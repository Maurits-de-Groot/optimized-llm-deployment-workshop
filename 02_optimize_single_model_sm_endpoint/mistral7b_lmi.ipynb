{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "72d0eb87-e8c2-4ac8-8eb8-29dbc58c0fca",
   "metadata": {},
   "source": [
    "## Deploying Mistral 7B with TensorRT-LLM through SageMaker LMI container and streaming outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35f32b20-181b-4d62-a62f-3b842b914c49",
   "metadata": {},
   "source": [
    "### 1. Import required packages, set up\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1d71184-316e-4a65-9a15-46f6a452feb7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install -U boto3 sagemaker --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9888b671-5175-48ee-a7ed-aeb833241bc8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "from sagemaker import Model, image_uris, serializers, deserializers\n",
    "\n",
    "role = sagemaker.get_execution_role()  # execution role for the endpoint\n",
    "sess = sagemaker.session.Session()  # sagemaker session for interacting with different AWS APIs\n",
    "region = sess._region_name  # region name of the current SageMaker Studio environment\n",
    "account_id = sess.account_id()  # account_id of the current SageMaker Studio environment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43369b1c-1876-4d04-9e39-f5ce03cb606a",
   "metadata": {},
   "source": [
    "### 2. Build SageMaker endpoint"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b802a3c-1c9a-462c-bdc7-2ff0724c4843",
   "metadata": {},
   "source": [
    "In this step, we will build SageMaker endpoint from scratch, using the AWS Large Model Inference (LMI) container.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9faeb68-3f0b-4684-ba43-fc0b2bc32645",
   "metadata": {},
   "source": [
    "#### 2.1. Get the container image URI\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e85951d-0756-41e7-a45a-0cf6bd9b9b03",
   "metadata": {},
   "source": [
    "[All available LMI container images](https://github.com/aws/deep-learning-containers/blob/master/available_images.md#large-model-inference-containers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "973f6771-e3d9-4d92-a8bd-3b51300ea286",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "image_uri = image_uris.retrieve(\n",
    "        # framework=\"djl-tensorrtllm\",\n",
    "        framework=\"djl-deepspeed\", # use this container version for vLLM, deepspeed and lmidist backends\n",
    "        region=sess.boto_session.region_name,\n",
    "        version=\"0.26.0\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "761056f1-127f-461d-98fd-0049f070f7f2",
   "metadata": {},
   "source": [
    "#### 2.2. Set up required enviroment variales and create SageMaker Model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcf3a8e2-2e54-4a59-992b-d86f3def7b3a",
   "metadata": {},
   "source": [
    "The model to be loaded, as well as all the common and specific LMI backend configurations can be set via environment variables. Instead of pulling from HF Model Hub, you can pull a model from an S3 bucket. You can also configure these parameters via a serving.properties file that you pass to the endpoint, which allows you to pass other artifacts along with it (even model artifacts) in a model folder. See [here](https://github.com/deepjavalibrary/djl-serving/blob/master/serving/docs/lmi_new/deployment_guide/configurations.md#container-and-model-configurations) for more details on server configuration, and see [here](https://github.com/deepjavalibrary/djl-serving/blob/master/serving/docs/lmi_new/deployment_guide/backend-selection.md) for guidance on what backend to select. Specific vLLM guidance can be found [here](https://github.com/deepjavalibrary/djl-serving/blob/master/serving/docs/lmi_new/user_guides/vllm_user_guide.md)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20bc6724-e9fd-437a-afdc-8533cae7cf8b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "env = {\n",
    "    'HF_MODEL_ID':'mistralai/Mistral-7B-Instruct-v0.1',\n",
    "    'OPTION_ROLLING_BATCH':'vllm',\n",
    "    'TENSOR_PARALLEL_DEGREE': 'max',\n",
    "    'OPTION_MAX_MODEL_LEN':'4000'\n",
    "}\n",
    "\n",
    "model = Model(image_uri=image_uri, role=role, env=env)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "886a4816-fe1e-4775-910c-3bc090e6c2cb",
   "metadata": {},
   "source": [
    "#### 2.3. Create SageMaker real-time endpoint\n",
    "\n",
    "We will deploy our model to a `g5.xlarge` instance, backed by a single 24GB A10G GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcd8039a-0f56-4ed3-87c6-1615d8328058",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "instance_type = \"ml.g5.xlarge\"\n",
    "endpoint_name = sagemaker.utils.name_from_base(\"lmi-model\")\n",
    "\n",
    "predictor = model.deploy(\n",
    "    initial_instance_count=1,\n",
    "    instance_type=instance_type,\n",
    "    endpoint_name=endpoint_name,\n",
    "    # container_startup_health_check_timeout=3600\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8420a73a-6355-4551-affb-65da3ef10996",
   "metadata": {},
   "source": [
    "### 3. Create prompt building utility functions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f33cfa99-22aa-43ca-a474-086d26e75a3c",
   "metadata": {},
   "source": [
    "Let's write a function that builds a prompt format to induce instruction-following behaviour. Note that this format is model dependant (the Mistral model was instruction-tuned with the [INST] tag, but other models may have see different special tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a1bb70a-2607-4e07-9b81-32e7369788b9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def build_mistral_prompt(instructions):\n",
    "    stop_token = \"</s>\"\n",
    "    start_token = \"<s>\"\n",
    "    startPrompt = f\"{start_token}[INST] \"\n",
    "    endPrompt = \" [/INST]\"\n",
    "    conversation = []\n",
    "    for index, instruction in enumerate(instructions):\n",
    "        if instruction[\"role\"] == \"system\" and index == 0:\n",
    "            conversation.append(f\"<<SYS>>\\n{instruction['content']}\\n<</SYS>>\\n\")\n",
    "        elif instruction[\"role\"] == \"user\":\n",
    "            conversation.append(instruction[\"content\"].strip())\n",
    "        else:\n",
    "            conversation.append(f\"{endPrompt} {instruction['content'].strip()} {stop_token}{startPrompt}\")\n",
    "\n",
    "    return startPrompt + \"\".join(conversation) + endPrompt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cfa10d2-0568-494e-9997-74611cabe7fe",
   "metadata": {},
   "source": [
    "And  another function to join our base system prompt and actual user request"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca2dfa94-ac80-48dc-9517-07325f37f5d1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_instructions(user_content):\n",
    "\n",
    "    '''\n",
    "    Note: We are creating a fresh user content everytime by initializing instructions for every user_content.\n",
    "    This is to avoid past user_content when you are inferencing multiple times with new ask everytime.\n",
    "    ''' \n",
    "\n",
    "    system_content = '''\n",
    "    You are a friendly and knowledgeable email marketing agent, Mr.MightyMark, working at AnyCompany. \n",
    "    Your goal is to send email to subscribers to help them understand the value of the new product and generate excitement for the launch.\n",
    "\n",
    "    Here are some tips on how to achieve your goal:\n",
    "\n",
    "    Be personal. Address each subscriber by name and use a friendly and conversational tone.\n",
    "    Be informative. Explain the key features and benefits of the new product in a clear and concise way.\n",
    "    Be persuasive. Highlight how the new product can solve the subscriber's problems or improve their lives.\n",
    "    Be engaging. Use emojis to make your emails more visually appealing and interesting to read.\n",
    "\n",
    "    By following these tips, you can use email marketing to help your company launch a successful software product.\n",
    "    '''\n",
    "\n",
    "    instructions = [\n",
    "        { \"role\": \"system\",\"content\": f\"{system_content} \"},\n",
    "    ]\n",
    "    \n",
    "    instructions.append({\"role\": \"user\", \"content\": f\"{user_content}\"})\n",
    "    \n",
    "    return instructions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f86a98f-47c2-4fba-a6ce-d37b573c34c5",
   "metadata": {},
   "source": [
    "Now we build and print our final prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8a5d26c-d407-4c37-918a-890d1dff06e9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "user_ask_1 = f'''\n",
    "AnyCompany recently announced new service launch named AnyCloud Internet Service.\n",
    "Write a short email about the product launch with Call to action to Alice Smith, whose email is alice.smith@example.com\n",
    "Mention the Coupon Code: EARLYB1RD to get 20% for 1st 3 months.\n",
    "'''\n",
    "instructions = get_instructions(user_ask_1)\n",
    "prompt = build_mistral_prompt(instructions)\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d790d332-e12e-4fd9-b720-20931136e1ed",
   "metadata": {},
   "source": [
    "### 4. Test inference by streaming model output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a809f517-197d-4197-96dc-56e241763cf4",
   "metadata": {},
   "source": [
    "First, we define a function that wraps around the sagemaker runtime's invoke_endpoint_with_response_stream method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abc98e11-ef44-4a35-b556-eb5821029de2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import boto3\n",
    "import json\n",
    "\n",
    "sagemaker_runtime = boto3.client('sagemaker-runtime')\n",
    "sagemaker_client = boto3.client('sagemaker')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92eeafb7-1d09-4966-b717-2518d0499b30",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_realtime_response_stream(sagemaker_runtime, endpoint_name, payload):\n",
    "    response_stream = sagemaker_runtime.invoke_endpoint_with_response_stream(\n",
    "        EndpointName=endpoint_name,\n",
    "        Body=json.dumps(payload), \n",
    "        ContentType=\"application/json\",\n",
    "        # CustomAttributes='accept_eula=false'\n",
    "    )\n",
    "    return response_stream"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "718a5c5b-41c1-45f9-aa2d-e836c6b8d89d",
   "metadata": {},
   "source": [
    "We will use a utility function to parse the output buffer from the SageMaker endpoint and print tokens as they are streamed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0852b23b-428a-4304-887f-102d16dee608",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from utils.LineIterator import LineIterator\n",
    "\n",
    "def print_response_stream(response_stream):\n",
    "    event_stream = response_stream.get('Body')\n",
    "    for line in LineIterator(event_stream):\n",
    "        print(line, end='')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "797bccde-c32a-4549-97f6-0aec9db4a681",
   "metadata": {},
   "source": [
    "Set your selected generation parameters, and create a payload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a232f919-e6ab-4d1c-b4e3-529e4317023c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "inference_params = {\n",
    "        \"do_sample\": True,\n",
    "        \"top_p\": 0.6,\n",
    "        \"temperature\": 0.9,\n",
    "        \"top_k\": 50,\n",
    "        \"max_new_tokens\": 512,\n",
    "    }\n",
    "\n",
    "payload = {\n",
    "    \"inputs\":  prompt,\n",
    "    \"parameters\": inference_params\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cf3f4b7-fee9-4769-8a1d-7b4e4394443b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "resp = get_realtime_response_stream(sagemaker_runtime, endpoint_name, payload)\n",
    "print_response_stream(resp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ed44e6a-4bc3-4868-a3b8-ac36e36d81e3",
   "metadata": {},
   "source": [
    "### 5. Clean up endpoint"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03109130-5a61-426a-a62e-5ea6eac93d2f",
   "metadata": {},
   "source": [
    "Finally, we terminate the endpoint so that it's not consuming resources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6295ebe0-c1a3-4976-a1ee-632250142352",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "predictor = sagemaker.predictor.Predictor(endpoint_name)\n",
    "predictor.delete_endpoint()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb7e367b-ae55-4484-bd6a-e75c294e883e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_p310",
   "language": "python",
   "name": "conda_pytorch_p310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
