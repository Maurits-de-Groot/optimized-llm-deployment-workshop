{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "72d0eb87-e8c2-4ac8-8eb8-29dbc58c0fca",
   "metadata": {},
   "source": [
    "## Deploying Mistral 7B with NeuronX on Inf2 through SageMaker LMI container and streaming outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35f32b20-181b-4d62-a62f-3b842b914c49",
   "metadata": {},
   "source": [
    "### 1. Import required packages, set up\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1d71184-316e-4a65-9a15-46f6a452feb7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install -U boto3 sagemaker --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9888b671-5175-48ee-a7ed-aeb833241bc8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sagemaker.config INFO - Not applying SDK defaults from location: /etc/xdg/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /home/ec2-user/.config/sagemaker/config.yaml\n"
     ]
    }
   ],
   "source": [
    "import sagemaker\n",
    "from sagemaker import Model, image_uris, serializers, deserializers\n",
    "\n",
    "role = sagemaker.get_execution_role()  # execution role for the endpoint\n",
    "sess = sagemaker.session.Session()  # sagemaker session for interacting with different AWS APIs\n",
    "region = sess._region_name  # region name of the current SageMaker Studio environment\n",
    "account_id = sess.account_id()  # account_id of the current SageMaker Studio environment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d1109b1-dc3a-4bf8-ac5d-2ea2bc9a6826",
   "metadata": {},
   "source": [
    "### 2. Prepare model artifacts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad9b8299-8df1-4d58-9502-b6dec5eb7ba5",
   "metadata": {},
   "source": [
    "This time around, we will set up the endpoint using a `serving.properties` file. In LMI container, we can pass artifacts to help setting up the model.\n",
    "* serving.properties (either this or env variables are required) - this defines the model server settings\n",
    "* `model.py` (optional) inference script. This would define the core inference logic\n",
    "* requirements.txt (optional): Any additional pip wheel need to install"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2613151f-fd40-44fb-b784-b1b239b75aa6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing serving.properties\n"
     ]
    }
   ],
   "source": [
    "%%writefile serving.properties\n",
    "engine=Python\n",
    "option.entryPoint=djl_python.transformers_neuronx\n",
    "option.model_id=mistralai/Mistral-7B-v0.1\n",
    "option.max_rolling_batch_size=4\n",
    "option.tensor_parallel_degree=2\n",
    "option.n_positions=512\n",
    "option.rolling_batch=auto\n",
    "option.model_loading_timeout=2400"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fa6c1d40-9405-41d5-9ed2-861b96bc3a80",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./\n",
      "./serving.properties\n"
     ]
    }
   ],
   "source": [
    "%%sh\n",
    "mkdir mymodel\n",
    "mv serving.properties mymodel/\n",
    "tar czvf mymodel.tar.gz -C mymodel/ .\n",
    "rm -rf mymodel"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43369b1c-1876-4d04-9e39-f5ce03cb606a",
   "metadata": {},
   "source": [
    "### 2. Build SageMaker endpoint"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b802a3c-1c9a-462c-bdc7-2ff0724c4843",
   "metadata": {},
   "source": [
    "In this step, we will build SageMaker endpoint from scratch, using the AWS Large Model Inference (LMI) container.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9faeb68-3f0b-4684-ba43-fc0b2bc32645",
   "metadata": {},
   "source": [
    "#### 2.1. Get the container image URI\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e85951d-0756-41e7-a45a-0cf6bd9b9b03",
   "metadata": {},
   "source": [
    "[All available LMI container images](https://github.com/aws/deep-learning-containers/blob/master/available_images.md#large-model-inference-containers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "973f6771-e3d9-4d92-a8bd-3b51300ea286",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "image_uri = image_uris.retrieve(\n",
    "        framework=\"djl-neuronx\", # use this container version for NeuronX on Inf2 or Trn\n",
    "        region=sess.boto_session.region_name,\n",
    "        version=\"0.26.0\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "761056f1-127f-461d-98fd-0049f070f7f2",
   "metadata": {},
   "source": [
    "#### 2.2. Upload artifact on S3 and create SageMaker model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcf3a8e2-2e54-4a59-992b-d86f3def7b3a",
   "metadata": {},
   "source": [
    "See [here](https://github.com/deepjavalibrary/djl-serving/blob/master/serving/docs/lmi_new/deployment_guide/configurations.md#container-and-model-configurations) for more details on server configuration, and see [here](https://github.com/deepjavalibrary/djl-serving/blob/master/serving/docs/lmi_new/deployment_guide/backend-selection.md) for guidance on what backend to select. Specific NeuronX guidance can be found [here](https://github.com/deepjavalibrary/djl-serving/blob/master/serving/docs/lmi_new/user_guides/tnx_user_guide.md)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "20bc6724-e9fd-437a-afdc-8533cae7cf8b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "S3 Code or Model tar ball uploaded to --- &gt; s3://sagemaker-us-west-2-241563328129/large-model-lmi/code/mymodel.tar.gz\n"
     ]
    }
   ],
   "source": [
    "s3_code_prefix = \"large-model-lmi/code\"\n",
    "bucket = sess.default_bucket()  # bucket to house artifacts\n",
    "code_artifact = sess.upload_data(\"mymodel.tar.gz\", bucket, s3_code_prefix)\n",
    "print(f\"S3 Code or Model tar ball uploaded to --- &gt; {code_artifact}\")\n",
    "\n",
    "model = Model(image_uri=image_uri, model_data=code_artifact, role=role)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "886a4816-fe1e-4775-910c-3bc090e6c2cb",
   "metadata": {},
   "source": [
    "#### 2.3. Create SageMaker real-time endpoint\n",
    "\n",
    "We will deploy our model to a `inf2.8xlarge` instance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcd8039a-0f56-4ed3-87c6-1615d8328058",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "instance_type = \"ml.inf2.8xlarge\"\n",
    "endpoint_name = sagemaker.utils.name_from_base(\"lmi-inf2-model\")\n",
    "\n",
    "model.deploy(\n",
    "    initial_instance_count=1,\n",
    "    instance_type=instance_type,\n",
    "    endpoint_name=endpoint_name,\n",
    "    container_startup_health_check_timeout=2400,\n",
    "    volume_size=256\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8420a73a-6355-4551-affb-65da3ef10996",
   "metadata": {},
   "source": [
    "### 3. Create prompt building utility functions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f33cfa99-22aa-43ca-a474-086d26e75a3c",
   "metadata": {},
   "source": [
    "Let's write a function that builds a prompt format to induce instruction-following behaviour. Note that this format is model dependant (the Mistral model was instruction-tuned with the [INST] tag, but other models may have see different special tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a1bb70a-2607-4e07-9b81-32e7369788b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_mistral_prompt(instructions):\n",
    "    stop_token = \"</s>\"\n",
    "    start_token = \"<s>\"\n",
    "    startPrompt = f\"{start_token}[INST] \"\n",
    "    endPrompt = \" [/INST]\"\n",
    "    conversation = []\n",
    "    for index, instruction in enumerate(instructions):\n",
    "        if instruction[\"role\"] == \"system\" and index == 0:\n",
    "            conversation.append(f\"\\n{instruction['content']}\\n\\n\")\n",
    "        elif instruction[\"role\"] == \"user\":\n",
    "            conversation.append(instruction[\"content\"].strip())\n",
    "        else:\n",
    "            conversation.append(f\"{endPrompt} {instruction['content'].strip()} {stop_token}{startPrompt}\")\n",
    "\n",
    "    return startPrompt + \"\".join(conversation) + endPrompt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cfa10d2-0568-494e-9997-74611cabe7fe",
   "metadata": {},
   "source": [
    "And  another function to join our base system prompt and actual user request"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca2dfa94-ac80-48dc-9517-07325f37f5d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_instructions(user_content):\n",
    "\n",
    "    '''\n",
    "    Note: We are creating a fresh user content everytime by initializing instructions for every user_content.\n",
    "    This is to avoid past user_content when you are inferencing multiple times with new ask everytime.\n",
    "    ''' \n",
    "\n",
    "    system_content = '''\n",
    "    You are a friendly and knowledgeable email marketing agent, Mr.MightyMark, working at AnyCompany. \n",
    "    Your goal is to send email to subscribers to help them understand the value of the new product and generate excitement for the launch.\n",
    "\n",
    "    Here are some tips on how to achieve your goal:\n",
    "\n",
    "    Be personal. Address each subscriber by name and use a friendly and conversational tone.\n",
    "    Be informative. Explain the key features and benefits of the new product in a clear and concise way.\n",
    "    Be persuasive. Highlight how the new product can solve the subscriber's problems or improve their lives.\n",
    "    Be engaging. Use emojis to make your emails more visually appealing and interesting to read.\n",
    "\n",
    "    By following these tips, you can use email marketing to help your company launch a successful software product.\n",
    "    '''\n",
    "\n",
    "    instructions = [\n",
    "        { \"role\": \"system\",\"content\": f\"{system_content} \"},\n",
    "    ]\n",
    "    \n",
    "    instructions.append({\"role\": \"user\", \"content\": f\"{user_content}\"})\n",
    "    \n",
    "    return instructions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f86a98f-47c2-4fba-a6ce-d37b573c34c5",
   "metadata": {},
   "source": [
    "Now we build and print our final prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8a5d26c-d407-4c37-918a-890d1dff06e9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "user_ask_1 = f'''\n",
    "AnyCompany recently announced new service launch named AnyCloud Internet Service.\n",
    "Write a short email about the product launch with Call to action to Alice Smith, whose email is alice.smith@example.com\n",
    "Mention the Coupon Code: EARLYB1RD to get 20% for 1st 3 months.\n",
    "'''\n",
    "instructions = get_instructions(user_ask_1)\n",
    "prompt = build_mistral_prompt(instructions)\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d790d332-e12e-4fd9-b720-20931136e1ed",
   "metadata": {},
   "source": [
    "### 4. Test inference by streaming model output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a809f517-197d-4197-96dc-56e241763cf4",
   "metadata": {},
   "source": [
    "First, we define a function that wraps around the sagemaker runtime's invoke_endpoint_with_response_stream method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abc98e11-ef44-4a35-b556-eb5821029de2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import boto3\n",
    "import json\n",
    "\n",
    "sagemaker_runtime = boto3.client('sagemaker-runtime')\n",
    "sagemaker_client = boto3.client('sagemaker')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92eeafb7-1d09-4966-b717-2518d0499b30",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_realtime_response_stream(sagemaker_runtime, endpoint_name, payload):\n",
    "    response_stream = sagemaker_runtime.invoke_endpoint_with_response_stream(\n",
    "        EndpointName=endpoint_name,\n",
    "        Body=json.dumps(payload), \n",
    "        ContentType=\"application/json\",\n",
    "        # CustomAttributes='accept_eula=false'\n",
    "    )\n",
    "    return response_stream"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "718a5c5b-41c1-45f9-aa2d-e836c6b8d89d",
   "metadata": {},
   "source": [
    "We will use a utility function to parse the output buffer from the SageMaker endpoint and print tokens as they are streamed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0852b23b-428a-4304-887f-102d16dee608",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from utils.LineIterator import LineIterator\n",
    "\n",
    "def print_response_stream(response_stream):\n",
    "    event_stream = response_stream.get('Body')\n",
    "    for line in LineIterator(event_stream):\n",
    "        print(line, end='')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "797bccde-c32a-4549-97f6-0aec9db4a681",
   "metadata": {},
   "source": [
    "Set your selected generation parameters, and create a payload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a232f919-e6ab-4d1c-b4e3-529e4317023c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "inference_params = {\n",
    "        \"do_sample\": True,\n",
    "        \"top_p\": 0.6,\n",
    "        \"temperature\": 0.9,\n",
    "        \"top_k\": 50,\n",
    "        \"max_new_tokens\": 512,\n",
    "    }\n",
    "\n",
    "payload = {\n",
    "    \"inputs\":  prompt,\n",
    "    \"parameters\": inference_params\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cf3f4b7-fee9-4769-8a1d-7b4e4394443b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "resp = get_realtime_response_stream(sagemaker_runtime, endpoint_name, payload)\n",
    "print_response_stream(resp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ed44e6a-4bc3-4868-a3b8-ac36e36d81e3",
   "metadata": {},
   "source": [
    "### 5. Clean up endpoint"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03109130-5a61-426a-a62e-5ea6eac93d2f",
   "metadata": {},
   "source": [
    "Finally, we terminate the endpoint so that it's not consuming resources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6295ebe0-c1a3-4976-a1ee-632250142352",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor = sagemaker.predictor.Predictor(endpoint_name)\n",
    "predictor.delete_endpoint()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_p310",
   "language": "python",
   "name": "conda_pytorch_p310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
